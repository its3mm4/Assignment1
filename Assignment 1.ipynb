{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0fc523b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: .\\corpus_ling_and_consentCLEAN.txt\n",
      " \n",
      " \n",
      "word count: 7441\n",
      " \n",
      " \n",
      "Lexical Diversity: 0.25413250907136137\n",
      " \n",
      " \n",
      "Top Ten Most Frequent Words and Their Counts:\n",
      "to: 192\n",
      "the: 186\n",
      "and: 183\n",
      "of: 181\n",
      "a: 178\n",
      "I: 165\n",
      "that: 148\n",
      "you: 121\n",
      "in: 91\n",
      "is: 82\n",
      " \n",
      " \n",
      "{'Lingthusiasm': 13, 'Transcript': 1, 'linguistics': 29, 'transcript': 1, 'readability': 1, 'enthusiastic': 2, 'University': 2, 'Roehampton': 1, 'announcements': 1, 'anniversary': 3, 'interesting': 7, 'independent': 1, 'linguistic': 6, 'linguistically': 1, 'patreoncom': 3, 'lingthusiasm': 3, 'financially': 1, 'appreciate': 3, 'supporting': 1, 'university': 3, 'restricted': 1, 'absolutely': 2, 'literature': 5, 'philosophy': 2, 'reasonable': 3, 'Linguistics': 1, 'continuity': 1, 'simultaneously': 1, 'secondyear': 1, 'challenging': 1, 'fascinating': 1, 'enthralling': 1, 'committing': 3, 'powerhouse': 1, 'undergraduate': 3, 'complicated': 2, 'investigate': 1, 'collection': 1, 'definition': 1, 'particular': 8, 'principles': 1, 'everything': 5, 'pickandchoose': 1, 'Contemporary': 1, 'principled': 1, 'representative': 1, 'Absolutely': 2, 'definitely': 4, 'distinctions': 1, 'subcorpora': 1, 'separately': 1, 'programmers': 1, 'lexicallynet': 1, 'specialist': 1, 'experience': 1, 'specialised': 1, 'determiner': 1, 'adjectives': 1, 'identifying': 1, 'programming': 1, 'fantastically': 1, 'partnership': 2, 'machinereadable': 1, 'newspapers': 4, 'recognition': 1, 'accurately': 2, 'disintegrate': 1, 'particularly': 3, 'historical': 1, 'representation': 1, 'mentioning': 1, 'suffragists': 3, 'suffragettes': 2, 'campaigners': 3, 'themselves': 1, 'disseminated': 1, 'perspective': 1, 'influential': 1, 'politically': 1, 'economically': 1, 'connection': 1, 'perception': 2, 'presenting': 1, 'immediately': 1, 'supportive': 1, 'historians': 1, 'international': 1, 'campaigned': 1, 'legislation': 1, 'parliament': 2, 'demonstrations': 1, 'differences': 2, 'disrupting': 1, 'techniques': 1, 'associated': 2, 'activities': 1, 'describing': 1, 'accomplish': 1, 'necessarily': 4, 'recognised': 1, 'dissertation': 1, 'trajectory': 1, 'unfortunately': 1, 'universities': 1, 'broadening': 1, 'morphology': 1, 'stylistics': 2, 'approaches': 1, 'comfortable': 1, 'hairraising': 1, 'generalisable': 1, 'sociolinguistics': 2, 'informative': 1, 'interested': 6, 'connections': 1, 'coinvestigator': 2, 'underworld': 1, 'fanfiction': 1, 'dissimilar': 1, 'contribution': 1, 'responding': 1, 'conferences': 1, 'interactions': 2, 'sociolinguistic': 1, 'dispreferred': 1, 'Unfortunately': 1, 'explanation': 1, 'affirmative': 1, 'interaction': 1, 'discussions': 1, 'potentially': 1, 'physically': 1, 'interpreting': 2, 'disappointed': 1, 'exboyfriends': 1, 'favourites': 1, 'expectations': 1, 'boundaries': 1, 'unprotected': 1, 'consequences': 1, 'imaginative': 1, 'reflections': 1, 'experienced': 1, 'empiricist': 1, 'opportunity': 1, 'nonerotica': 1, 'werewolves': 3, 'permission': 2, 'articulate': 1, 'identities': 2, 'experiences': 4, 'homonormativity': 3, 'increasingly': 1, 'behaviours': 1, 'relatively': 1, 'anthropology': 1, 'neuroscience': 1, 'ethnography': 1, 'scientific': 1, 'applications': 2, 'communication': 1, 'lingthusiasmcom': 2, 'SoundCloud': 1, 'GretchenAMcC': 1, 'AllThingsLinguisticcom': 1, 'Superlinguo': 1, 'mixosaurus': 2, 'translation': 1, 'especially': 1, 'Production': 1, 'McCullough': 1, 'lingthusiastic': 1}\n",
      " \n",
      " \n",
      "Longest Sentence:  I particularly like Aimee Bailey’s work on homonormativity in queer women’s media because she started out looking at sex advice columns for queer, bi, and lesbian women’s websites, and then was increasingly interested in, okay, what are they doing about, say, bi women who may have sexual experiences and desires and so on with people of different genders, and how do we make room for these desires and experiences and so on in a community focused on women who have sex with women.\n",
      "Number of Words in Longest Sentence:  98\n",
      " \n",
      " \n",
      "Original Longest Sentence:  I particularly like Aimee Bailey’s work on homonormativity in queer women’s media because she started out looking at sex advice columns for queer, bi, and lesbian women’s websites, and then was increasingly interested in, okay, what are they doing about, say, bi women who may have sexual experiences and desires and so on with people of different genders, and how do we make room for these desires and experiences and so on in a community focused on women who have sex with women.\n",
      " \n",
      "Stemmed Longest Sentence:  i particularli like aime bailey ’ s work on homonorm in queer women ’ s media becaus she start out look at sex advic column for queer , bi , and lesbian women ’ s websit , and then wa increasingli interest in , okay , what are they do about , say , bi women who may have sexual experi and desir and so on with peopl of differ gender , and how do we make room for these desir and experi and so on in a commun focus on women who have sex with women .\n"
     ]
    }
   ],
   "source": [
    "#this is everything that im importing\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#I have already downlaoded this.\n",
    "from nltk.stem import PorterStemmer\n",
    "#I have already downlaoded this.\n",
    "\n",
    "\n",
    "#FUNCTION FOR COUNTING WORDS IN FILE\n",
    "def count_words_in_file(file_path):\n",
    "\n",
    "    with open(file_path, 'r', encoding = \"utf8\") as file:\n",
    "            text = file.read()\n",
    "            #this splits/tokenizes the text\n",
    "            words = text.split()\n",
    "            #this counts the length in words and then returns the length.\n",
    "            word_count = len(words)\n",
    "            return word_count\n",
    "\n",
    "#LEXDIVERS\n",
    "def calculate_lexical_diversity(file_path):\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Calculate the number of tokens\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # Calculate the number of unique words (types)\n",
    "    unique_words = set(tokens)\n",
    "    num_unique_words = len(unique_words)\n",
    "    \n",
    "    # Calculate and return the lexical diversity\n",
    "    lexical_diversity = num_unique_words / num_tokens\n",
    "    return lexical_diversity\n",
    "\n",
    "#Function for 1o most frequent words and their counts\n",
    "def ten_freq(file_path):\n",
    "    # Open the file and read its content\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get the top ten most frequent words\n",
    "    top_ten_words = word_counts.most_common(10)\n",
    "\n",
    "    return top_ten_words\n",
    "\n",
    "#Function for ten finding the words with ten characters or more and how many times they appear in file.\n",
    "def count_long_words(file_path):\n",
    "    word_count = {}\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        for line in file:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                # Remove punctuation and consider only alphanumeric characters\n",
    "                word = ''.join(filter(str.isalnum, word))\n",
    "\n",
    "                if len(word) >= 10:\n",
    "                    word_count[word] = word_count.get(word, 0) + 1\n",
    "\n",
    "    return word_count\n",
    "\n",
    "#Function for longest sentence and number of words.\n",
    "def longest_sent(file_path):\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # these are the things that mark sentence boundaries\n",
    "    sentence_delimiters = '.!?'\n",
    "\n",
    "    # Tokenize text into sentences based on specified delimiters\n",
    "    sentences = [sentence.strip() for sentence in nltk.sent_tokenize(text) if sentence.strip()]\n",
    "\n",
    "    longest_sentence = max(sentences, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "    num_words = len(word_tokenize(longest_sentence))\n",
    "\n",
    "    return longest_sentence, num_words\n",
    "\n",
    "#Function for stemming longest sent using the \n",
    "def stem_longest_sentence(longest_sentence):\n",
    "    # Initialize the Porter Stemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    # Tokenize the longest sentence into words\n",
    "    words = word_tokenize(longest_sentence)\n",
    "\n",
    "    # Apply stemming to each word\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Join the stemmed words to form the stemmed sentence\n",
    "    stemmed_sentence = ' '.join(stemmed_words)\n",
    "\n",
    "    return stemmed_sentence\n",
    "\n",
    "\n",
    "#KEEP THIS BETWEEN FUNCTIONS AND CALLING THEM    \n",
    "file_path = input(\"File path: \")\n",
    "#FUNCTIONS ABOVE, CALLING THEM BELOW\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#CALLING WORD COUNT\n",
    "word_count = count_words_in_file(file_path)\n",
    "print(\"word count:\", word_count)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#CALLING LEXDIVERSITY\n",
    "diversity = calculate_lexical_diversity(file_path)\n",
    "print(\"Lexical Diversity:\", diversity)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#Calling ten most frequent words and their counts\n",
    "top_words_and_counts = ten_freq(file_path)\n",
    "print(\"Top Ten Most Frequent Words and Their Counts:\")\n",
    "for word, count in top_words_and_counts:\n",
    "    print(f\"{word}: {count}\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#calling words >10 and their counts\n",
    "result = count_long_words(file_path)\n",
    "print(result)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#Calling the function for longest setnence\n",
    "result_sentence, result_num_words = longest_sent(file_path)\n",
    "print(\"Longest Sentence: \", result_sentence)\n",
    "print(\"Number of Words in Longest Sentence: \", result_num_words)\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "#calling stemmer\n",
    "result_sentence, _ = longest_sent(file_path)\n",
    "stemmed_result = stem_longest_sentence(result_sentence)\n",
    "print(\"Original Longest Sentence: \", result_sentence)\n",
    "print(\" \")\n",
    "print(\"Stemmed Longest Sentence: \", stemmed_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d32a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the regular expression code that I used to clean the subcorpuses of their html data\n",
    "#CLEANS FILE\n",
    "#has been tested on lingthusiasm txt and works\n",
    "\n",
    "#imports the regex stuff.\n",
    "import re\n",
    "\n",
    "#this function gets us to specify the file we wish to clean, cleans it as a new file and lets us name it. \n",
    "#the output is in Assignment1 folder.\n",
    "def clean_html_file(input_file_path):\n",
    "    \n",
    "    with open(input_file_path, \"r\", encoding = \"utf8\") as file:\n",
    "        file_contents = file.read()\n",
    "    \n",
    "    pattern1 = r'\\[.*?\\]'\n",
    "    pattern2 = r'<.*?>'\n",
    "    \n",
    "    cleaned_content = re.sub(pattern1, '', file_contents)\n",
    "    cleaned_content = re.sub(pattern2, '', cleaned_content)\n",
    "\n",
    "    new_file_name = input(\"New name for cleaned .txt file: \")\n",
    "    \n",
    "    with open(new_file_name, \"w\", encoding = \"utf8\") as cleaned_file:\n",
    "        cleaned_file.write(cleaned_content)\n",
    "    \n",
    "    print(\"Clean .txt file is \" , new_file_name , \" and is in Assignment1 folder.\")\n",
    "    \n",
    "input_file_path = input(\"Input file path for target file:\")\n",
    "clean_html_file(input_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
